[
    {
        "title": "Differential Transformer",
        "date": "2025-01-11",
        "description": "A deep dive into Differential Attention and how it reduces noise allocation in transformer architectures by subtracting two softmax attention maps.",
        "badge": "PAPER REVIEW",
        "category": "Attention",
        "link": "/laundryroom/notes/differential_transformer.html"
    },
    {
        "title": "Hierarchial Reasoning Models",
        "date": "<built-in method date of datetime.datetime object at 0x000002507E4076F0>",
        "description": "A walkthrough of our MCTS implementation. We discuss distributed training challenges and compare performance metrics against DeepMind's reported figures.",
        "link": "notes/hierarchial_reasoning_models.html"
    },
    {
        "title": "Attention sinks",
        "date": "2025-12-23",
        "description": "Empty",
        "link": "notes/attention_sinks.html"
    },
    {
        "title": "The Platonic Representation Hypothesis",
        "date": "2025-12-23",
        "description": "Empty",
        "link": "notes/the_platonic_representation_hypothesis.html"
    },
    {
        "title": "The Platonic Representation Hypothesis",
        "date": "2025-12-23",
        "description": "Empty",
        "link": "notes/the_platonic_representation_hypothesis.html"
    }
]